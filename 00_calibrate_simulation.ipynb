{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# calibrate_simulation\n",
    "\n",
    "> A Metamodel-Based General-purpose Calibration Tool for Simulation Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#| default_exp calibrate_simulation\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from enum import Enum\n",
    "class OptimizerType(Enum):\n",
    "    GUROBI = 1\n",
    "    OR_TOOLS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models \n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "\n",
    "from ortools.linear_solver import pywraplp\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CalibrateSimulation:\n",
    "\n",
    "\n",
    "    def __init__(self,optimizer_type : OptimizerType):\n",
    "        self.num_hidden_nodes = 1000\n",
    "        self.optimizer_type = optimizer_type\n",
    "        return\n",
    "\n",
    "    def train_model(self, x_training, y_training, X_validation, y_validation):\n",
    "        self.num_input_cols = x_training.shape[1]\n",
    "        self.num_ouptup_cols = y_training.shape[1]\n",
    "\n",
    "        NN_model = models.Sequential()\n",
    "        NN_model.add(layers.Dense(self.num_hidden_nodes,activation='relu'))  #One hidden layer with 1000 nodes\n",
    "        NN_model.add(layers.Dense(self.num_ouptup_cols))\n",
    "        NN_model.compile(optimizer='adam', loss='mean_absolute_error')\n",
    "        chk = ModelCheckpoint('NN_model',monitor='val_loss',save_best_only=True,mode='min',verbose=1)\n",
    "        self.history = NN_model.fit(x_training,y_training,epochs=100,callbacks=[chk],validation_data=(X_validation,y_validation))\n",
    "\n",
    "        self.NN_model = tf.keras.models.load_model('NN_model')\n",
    "\n",
    "        self.weights_1 = np.array(self.NN_model.get_weights()[0])\n",
    "        self.weights_2 = np.array(self.NN_model.get_weights()[1])\n",
    "        self.weights_3 = np.array(self.NN_model.get_weights()[2])\n",
    "        self.weights_4 = np.array(self.NN_model.get_weights()[3])\n",
    "\n",
    "    def solve_optimization(self, target_val, lower_bound, upper_bound):\n",
    "        self.lower_bound = lower_bound\n",
    "        self.upper_bound = upper_bound\n",
    "        if self.optimizer_type == OptimizerType.GUROBI :\n",
    "            return self._solve_optimization_gurobi(target_val)\n",
    "        else :\n",
    "            return self._solve_optimization_or_tools(target_val)\n",
    "\n",
    "        \n",
    "    def _solve_optimization_gurobi(self, target_val):\n",
    "        # Create initial model\n",
    "        model = gp.Model('project')\n",
    "        model.Params.LogToConsole = 0\n",
    "        model.Params.MIPFocus = 1\n",
    "        model.setParam('MIPGap', 0.000001)\n",
    "\n",
    "        #Index\n",
    "\n",
    "        I = self.num_input_cols\n",
    "        J = self.num_ouptup_cols\n",
    "        L = self.num_hidden_nodes\n",
    "\n",
    "        #Parameters\n",
    "\n",
    "        M = self.num_hidden_nodes\n",
    "        h = target_val\n",
    "        w1 = self.weights_1\n",
    "        b1 = self.weights_2\n",
    "        w2 = self.weights_3\n",
    "        b2 = self.weights_4\n",
    "\n",
    "        #Decision Variables\n",
    "\n",
    "        x = model.addVars(I,vtype=GRB.CONTINUOUS,name='x',lb=self.lower_bound,ub=self.upper_bound)  #the value of parameter i\n",
    "        y = model.addVars(L, vtype=GRB.CONTINUOUS,name='y')  #the hidden node value in output j for node l\n",
    "        z = model.addVars(J, vtype=GRB.CONTINUOUS,name='z')  #the model value for output j\n",
    "        u = model.addVars(L, vtype=GRB.BINARY,name='u')  #the binary indicator for activation of l th hidden node\n",
    "        d = model.addVars(J, vtype=GRB.CONTINUOUS,name='d')  #the difference value for output j\n",
    "\n",
    "        # Constraints:\n",
    "\n",
    "        model.addConstrs((gp.quicksum(w1[i, l] * x[i] for i in range(I)) + b1[l] <= y[l] for l in range(L)),name='FirstConsts')\n",
    "        model.addConstrs((gp.quicksum(w1[i, l] * x[i] for i in range(I)) + b1[l] + M * (1 - u[l]) >= y[l] for l in range(L)),name='SecondConsts')\n",
    "        model.addConstrs((y[l] <= M * u[l] for l in range(L)),name='ThirdCOnsts')\n",
    "        model.addConstrs((gp.quicksum(y[l] * w2[l, j] for l in range(L)) + b2[j] == z[j]for j in range(J)),name='FourthConsts')\n",
    "        model.addConstrs((d[j] >= z[j] - h[j] for j in range(J)),name='FifthConsts')\n",
    "        model.addConstrs((d[j] >= h[j] - z[j] for j in range(J)),name='SixthConsts')\n",
    "\n",
    "        # Set global sense for ALL objectives\n",
    "        model.ModelSense = GRB.MINIMIZE\n",
    "\n",
    "        obj = gp.quicksum(d[j] for j in range(J))\n",
    "        model.setObjective(obj)\n",
    "\n",
    "        # Optimize\n",
    "\n",
    "        model.optimize()\n",
    "\n",
    "        parameter_list = []\n",
    "        for i in range(I):\n",
    "            parameter_list.append(x[i].x)\n",
    "\n",
    "        output_list = []\n",
    "        for i in range(J):\n",
    "            output_list.append(z[i].x)\n",
    "        \"\"\"\n",
    "        for v in model.getVars():\n",
    "            print('%s %g' % (v.varName, v.x))\n",
    "                \n",
    "        print('Obj: %g' % obj.getValue())\n",
    "        \"\"\"\n",
    "\n",
    "        return parameter_list, output_list\n",
    "    \n",
    "    def _solve_optimization_or_tools(self, target_val):\n",
    "        solver = pywraplp.Solver.CreateSolver('SCIP')\n",
    "\n",
    "        mip_gap = 0.000001\n",
    "\n",
    "        #model = pywraplp.Solver('model', pywraplp.Solver.SCIP_MIXED_INTEGER_PROGRAMMING)\n",
    "\n",
    "        solverParams = pywraplp.MPSolverParameters()\n",
    "        solverParams.SetDoubleParam(solverParams.RELATIVE_MIP_GAP, mip_gap)\n",
    "\n",
    "        #Index\n",
    "\n",
    "        I = self.num_input_cols\n",
    "        J = self.num_ouptup_cols\n",
    "        L = self.num_hidden_nodes\n",
    "\n",
    "        #Parameters\n",
    "\n",
    "        M = self.num_hidden_nodes\n",
    "        h = target_val\n",
    "        w1 = self.weights_1\n",
    "        b1 = self.weights_2\n",
    "        w2 = self.weights_3\n",
    "        b2 = self.weights_4\n",
    "\n",
    "        lb = [1.0, 0.5, 5.0, 1.0, 0.4, 390.0]\n",
    "        ub = [2.5, 1.0, 10.0, 5.0, 0.8, 760.0]\n",
    "        infinity = solver.infinity()\n",
    "        x_vars = {}\n",
    "        y_vars = {}\n",
    "        z_vars = {}\n",
    "        u_vars = {}\n",
    "        d_vars = {}\n",
    "\n",
    "        for i in range(I):\n",
    "            x_vars[i] = solver.NumVar(lb=self.lower_bound[i], ub=self.upper_bound[i], name=f'x[{i}]')\n",
    "        for i in range(L):\n",
    "            y_vars[i] = solver.NumVar(lb=0.0, ub=infinity, name=f'y[{i}]')\n",
    "        for i in range(J):\n",
    "            z_vars[i] = solver.NumVar(lb=0.0, ub=infinity, name=f'z[{i}]')\n",
    "        for i in range(L):\n",
    "            u_vars[i] = solver.BoolVar(name=f'u[{i}]')\n",
    "        for i in range(J):\n",
    "            d_vars[i] = solver.NumVar(lb=0.0, ub=infinity, name=f'd[{i}]')\n",
    "\n",
    "        for l in range(L):\n",
    "            constraint_expr = [w1[i,l] * x_vars[i] for i in range(I)]\n",
    "            solver.Add(sum(constraint_expr) + b1[l] <= y_vars[l])\n",
    "\n",
    "        for l in range(L):\n",
    "            constraint_expr = [w1[i,l] * x_vars[i] for i in range(I)]\n",
    "            solver.Add(sum(constraint_expr) + b1[l] + M * (1 - u_vars[l]) >= y_vars[l])\n",
    "\n",
    "        for l in range(L):\n",
    "            solver.Add(M * u_vars[l] >= y_vars[l])\n",
    "\n",
    "        for j in range(J):\n",
    "            constraint_expr = [w2[l,j] * y_vars[l] for l in range(L)]\n",
    "            solver.Add(sum(constraint_expr) + b2[j] == z_vars[j])\n",
    "\n",
    "        for j in range(J):\n",
    "            solver.Add(z_vars[j] - h[j] <= d_vars[j])\n",
    "\n",
    "        for j in range(J):\n",
    "            solver.Add(h[j] - z_vars[j] <= d_vars[j])\n",
    "\n",
    "\n",
    "        obj_expr = [d_vars[j] for j in range(J)]\n",
    "        solver.Minimize(solver.Sum(obj_expr))\n",
    "\n",
    "        status = solver.Solve()\n",
    "\n",
    "        self.parameter_list = [x_vars[i].solution_value() for i in range(I)]\n",
    "\n",
    "\n",
    "        self.output_list = [z_vars[j].solution_value() for j in range(J)]\n",
    "\n",
    "        return self.parameter_list, self.output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = pd.read_csv('testing/data/output-seed_1337-30000.csv')\n",
    "#NORMALIZATION\n",
    "M.drop(M.columns[[8,12,13,16,17,18,21]], axis = 1, inplace= True)\n",
    "for i in range(6,15):\n",
    "    M.iloc[:,i] = (M.iloc[:,i] - M.iloc[:,i].min())/(M.iloc[:,i].max() - M.iloc[:,i].min())\n",
    "    target_sample = M.sample(100)\n",
    "M = M.drop(target_sample.index)\n",
    "M.reset_index(inplace=True)\n",
    "M.drop(M.columns[0], axis = 1, inplace=True)\n",
    "M.to_csv('testing/data/OutputMinMaxScaledResults30000.csv')\n",
    "target_sample.to_csv('testing/data/TargetSample.csv')\n",
    "\n",
    "M_train = M.sample(frac=0.7,  random_state=1337)\n",
    "M_rest = M.drop(M_train.index)\n",
    "M_validation = M_rest.sample(frac=0.5,  random_state=1337)\n",
    "M_test = M_rest.drop(M_validation.index)\n",
    "\n",
    "X_training = np.array(M_train.iloc[:,:6])\n",
    "X_validation = np.array(M_validation.iloc[:,:6])\n",
    "X_test = np.array(M_test.iloc[:,:6])\n",
    "\n",
    "Y_training = np.array(M_train.iloc[:,6:])\n",
    "Y_validation = np.array(M_validation.iloc[:,6:])\n",
    "Y_test = np.array(M_test.iloc[:,6:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Target_Sample = pd.read_csv('testing/data/TargetSample.csv', index_col=0).iloc[:,[6,7,8,9,10,11,12,13,14]]\n",
    "Target_Sample.reset_index(inplace=True)\n",
    "Target_Sample.drop(Target_Sample.columns[0], axis = 1, inplace=True)\n",
    "target_val = np.array(Target_Sample[0:1]).tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "648/655 [============================>.] - ETA: 0s - loss: 1.3731\n",
      "Epoch 1: val_loss improved from inf to 0.86117, saving model to NN_model\n",
      "INFO:tensorflow:Assets written to: NN_model\\assets\n",
      "655/655 [==============================] - 2s 2ms/step - loss: 1.3718 - val_loss: 0.8612\n",
      "Epoch 2/100\n",
      "652/655 [============================>.] - ETA: 0s - loss: 1.0440\n",
      "Epoch 2: val_loss improved from 0.86117 to 0.81997, saving model to NN_model\n",
      "INFO:tensorflow:Assets written to: NN_model\\assets\n",
      "655/655 [==============================] - 1s 2ms/step - loss: 1.0432 - val_loss: 0.8200\n",
      "Epoch 3/100\n",
      "647/655 [============================>.] - ETA: 0s - loss: 0.9683\n",
      "Epoch 3: val_loss improved from 0.81997 to 0.51361, saving model to NN_model\n",
      "INFO:tensorflow:Assets written to: NN_model\\assets\n",
      "655/655 [==============================] - 1s 2ms/step - loss: 0.9639 - val_loss: 0.5136\n",
      "Epoch 4/100\n",
      "645/655 [============================>.] - ETA: 0s - loss: 0.8392\n",
      "Epoch 4: val_loss did not improve from 0.51361\n",
      "655/655 [==============================] - 1s 1ms/step - loss: 0.8365 - val_loss: 0.9342\n",
      "Epoch 5/100\n",
      "641/655 [============================>.] - ETA: 0s - loss: 0.7261\n",
      "Epoch 5: val_loss improved from 0.51361 to 0.44311, saving model to NN_model\n",
      "INFO:tensorflow:Assets written to: NN_model\\assets\n",
      "655/655 [==============================] - 1s 2ms/step - loss: 0.7234 - val_loss: 0.4431\n",
      "Epoch 6/100\n",
      "607/655 [==========================>...] - ETA: 0s - loss: 0.6823\n",
      "Epoch 6: val_loss did not improve from 0.44311\n",
      "655/655 [==============================] - 1s 1ms/step - loss: 0.6699 - val_loss: 0.5105\n",
      "Epoch 7/100\n",
      "654/655 [============================>.] - ETA: 0s - loss: 0.5564\n",
      "Epoch 7: val_loss did not improve from 0.44311\n",
      "655/655 [==============================] - 1s 1ms/step - loss: 0.5564 - val_loss: 0.7869\n",
      "Epoch 8/100\n",
      "648/655 [============================>.] - ETA: 0s - loss: 0.4869\n",
      "Epoch 8: val_loss improved from 0.44311 to 0.33902, saving model to NN_model\n",
      "INFO:tensorflow:Assets written to: NN_model\\assets\n",
      "655/655 [==============================] - 1s 2ms/step - loss: 0.4859 - val_loss: 0.3390\n",
      "Epoch 9/100\n",
      "650/655 [============================>.] - ETA: 0s - loss: 0.4091\n",
      "Epoch 9: val_loss did not improve from 0.33902\n",
      "655/655 [==============================] - 1s 1ms/step - loss: 0.4093 - val_loss: 0.3917\n",
      "Epoch 10/100\n",
      "629/655 [===========================>..] - ETA: 0s - loss: 0.3273\n",
      "Epoch 10: val_loss improved from 0.33902 to 0.29194, saving model to NN_model\n",
      "INFO:tensorflow:Assets written to: NN_model\\assets\n",
      "655/655 [==============================] - 1s 2ms/step - loss: 0.3264 - val_loss: 0.2919\n",
      "Epoch 11/100\n",
      "609/655 [==========================>...] - ETA: 0s - loss: 0.2865\n",
      "Epoch 11: val_loss improved from 0.29194 to 0.15556, saving model to NN_model\n",
      "INFO:tensorflow:Assets written to: NN_model\\assets\n",
      "655/655 [==============================] - 1s 2ms/step - loss: 0.2848 - val_loss: 0.1556\n",
      "Epoch 12/100\n",
      "643/655 [============================>.] - ETA: 0s - loss: 0.2675\n",
      "Epoch 12: val_loss did not improve from 0.15556\n",
      "655/655 [==============================] - 1s 1ms/step - loss: 0.2663 - val_loss: 0.1782\n",
      "Epoch 13/100\n",
      "612/655 [===========================>..] - ETA: 0s - loss: 0.1881\n",
      "Epoch 13: val_loss did not improve from 0.15556\n",
      "655/655 [==============================] - 1s 1ms/step - loss: 0.1875 - val_loss: 0.1924\n",
      "Epoch 14/100\n",
      "612/655 [===========================>..] - ETA: 0s - loss: 0.1580\n",
      "Epoch 14: val_loss did not improve from 0.15556\n",
      "655/655 [==============================] - 1s 1ms/step - loss: 0.1586 - val_loss: 0.1966\n",
      "Epoch 15/100\n",
      "609/655 [==========================>...] - ETA: 0s - loss: 0.1514\n",
      "Epoch 15: val_loss improved from 0.15556 to 0.13599, saving model to NN_model\n",
      "INFO:tensorflow:Assets written to: NN_model\\assets\n",
      "655/655 [==============================] - 1s 2ms/step - loss: 0.1496 - val_loss: 0.1360\n",
      "Epoch 16/100\n",
      "621/655 [===========================>..] - ETA: 0s - loss: 0.1089\n",
      "Epoch 16: val_loss did not improve from 0.13599\n",
      "655/655 [==============================] - 1s 1ms/step - loss: 0.1090 - val_loss: 0.1464\n",
      "Epoch 17/100\n",
      "653/655 [============================>.] - ETA: 0s - loss: 0.0985\n",
      "Epoch 17: val_loss improved from 0.13599 to 0.12128, saving model to NN_model\n",
      "INFO:tensorflow:Assets written to: NN_model\\assets\n",
      "655/655 [==============================] - 1s 2ms/step - loss: 0.0985 - val_loss: 0.1213\n",
      "Epoch 18/100\n",
      "600/655 [==========================>...] - ETA: 0s - loss: 0.0813\n",
      "Epoch 18: val_loss improved from 0.12128 to 0.11832, saving model to NN_model\n",
      "INFO:tensorflow:Assets written to: NN_model\\assets\n",
      "655/655 [==============================] - 1s 2ms/step - loss: 0.0830 - val_loss: 0.1183\n",
      "Epoch 19/100\n",
      "649/655 [============================>.] - ETA: 0s - loss: 0.0737\n",
      "Epoch 19: val_loss improved from 0.11832 to 0.07888, saving model to NN_model\n",
      "INFO:tensorflow:Assets written to: NN_model\\assets\n",
      "655/655 [==============================] - 1s 2ms/step - loss: 0.0738 - val_loss: 0.0789\n",
      "Epoch 20/100\n",
      "632/655 [===========================>..] - ETA: 0s - loss: 0.0690\n",
      "Epoch 20: val_loss improved from 0.07888 to 0.07536, saving model to NN_model\n",
      "INFO:tensorflow:Assets written to: NN_model\\assets\n",
      "655/655 [==============================] - 1s 2ms/step - loss: 0.0684 - val_loss: 0.0754\n",
      "Epoch 21/100\n",
      "616/655 [===========================>..] - ETA: 0s - loss: 0.0620\n",
      "Epoch 21: val_loss did not improve from 0.07536\n",
      "655/655 [==============================] - 1s 1ms/step - loss: 0.0618 - val_loss: 0.1018\n",
      "Epoch 22/100\n",
      "640/655 [============================>.] - ETA: 0s - loss: 0.0608\n",
      "Epoch 22: val_loss did not improve from 0.07536\n",
      "655/655 [==============================] - 1s 1ms/step - loss: 0.0605 - val_loss: 0.1247\n",
      "Epoch 23/100\n",
      "611/655 [==========================>...] - ETA: 0s - loss: 0.0556\n",
      "Epoch 23: val_loss improved from 0.07536 to 0.05544, saving model to NN_model\n",
      "INFO:tensorflow:Assets written to: NN_model\\assets\n",
      "655/655 [==============================] - 1s 2ms/step - loss: 0.0557 - val_loss: 0.0554\n",
      "Epoch 24/100\n",
      "616/655 [===========================>..] - ETA: 0s - loss: 0.0515\n",
      "Epoch 24: val_loss improved from 0.05544 to 0.05339, saving model to NN_model\n",
      "INFO:tensorflow:Assets written to: NN_model\\assets\n",
      "655/655 [==============================] - 1s 2ms/step - loss: 0.0512 - val_loss: 0.0534\n",
      "Epoch 25/100\n",
      "631/655 [===========================>..] - ETA: 0s - loss: 0.0507\n",
      "Epoch 25: val_loss did not improve from 0.05339\n",
      "655/655 [==============================] - 1s 1ms/step - loss: 0.0503 - val_loss: 0.0680\n",
      "Epoch 26/100\n",
      "614/655 [===========================>..] - ETA: 0s - loss: 0.0484\n",
      "Epoch 26: val_loss improved from 0.05339 to 0.04011, saving model to NN_model\n",
      "INFO:tensorflow:Assets written to: NN_model\\assets\n",
      "655/655 [==============================] - 1s 2ms/step - loss: 0.0485 - val_loss: 0.0401\n",
      "Epoch 27/100\n",
      "623/655 [===========================>..] - ETA: 0s - loss: 0.0435\n",
      "Epoch 27: val_loss did not improve from 0.04011\n",
      "655/655 [==============================] - 1s 1ms/step - loss: 0.0431 - val_loss: 0.0554\n",
      "Epoch 28/100\n",
      "615/655 [===========================>..] - ETA: 0s - loss: 0.0417\n",
      "Epoch 28: val_loss did not improve from 0.04011\n",
      "655/655 [==============================] - 1s 1ms/step - loss: 0.0412 - val_loss: 0.1024\n",
      "Epoch 29/100\n",
      "647/655 [============================>.] - ETA: 0s - loss: 0.0417\n",
      "Epoch 29: val_loss did not improve from 0.04011\n",
      "655/655 [==============================] - 1s 1ms/step - loss: 0.0415 - val_loss: 0.0730\n",
      "Epoch 30/100\n",
      "602/655 [==========================>...] - ETA: 0s - loss: 0.0388\n",
      "Epoch 30: val_loss did not improve from 0.04011\n",
      "655/655 [==============================] - 1s 1ms/step - loss: 0.0388 - val_loss: 0.0635\n",
      "Epoch 31/100\n",
      "617/655 [===========================>..] - ETA: 0s - loss: 0.0387\n",
      "Epoch 31: val_loss did not improve from 0.04011\n",
      "655/655 [==============================] - 1s 1ms/step - loss: 0.0384 - val_loss: 0.0561\n",
      "Epoch 32/100\n",
      "603/655 [==========================>...] - ETA: 0s - loss: 0.0372\n",
      "Epoch 32: val_loss improved from 0.04011 to 0.03398, saving model to NN_model\n",
      "INFO:tensorflow:Assets written to: NN_model\\assets\n",
      "655/655 [==============================] - 1s 2ms/step - loss: 0.0371 - val_loss: 0.0340\n",
      "Epoch 33/100\n",
      "616/655 [===========================>..] - ETA: 0s - loss: 0.0340\n",
      "Epoch 33: val_loss did not improve from 0.03398\n",
      "655/655 [==============================] - 1s 1ms/step - loss: 0.0341 - val_loss: 0.0595\n",
      "Epoch 34/100\n",
      "617/655 [===========================>..] - ETA: 0s - loss: 0.0355\n",
      "Epoch 34: val_loss did not improve from 0.03398\n",
      "655/655 [==============================] - 1s 1ms/step - loss: 0.0352 - val_loss: 0.0392\n",
      "Epoch 35/100\n",
      "646/655 [============================>.] - ETA: 0s - loss: 0.0337\n",
      "Epoch 35: val_loss did not improve from 0.03398\n",
      "655/655 [==============================] - 1s 1ms/step - loss: 0.0342 - val_loss: 0.0672\n",
      "Epoch 36/100\n",
      "643/655 [============================>.] - ETA: 0s - loss: 0.0346\n",
      "Epoch 36: val_loss did not improve from 0.03398\n",
      "655/655 [==============================] - 1s 1ms/step - loss: 0.0346 - val_loss: 0.0835\n",
      "Epoch 37/100\n",
      "613/655 [===========================>..] - ETA: 0s - loss: 0.0352\n",
      "Epoch 37: val_loss improved from 0.03398 to 0.02544, saving model to NN_model\n",
      "INFO:tensorflow:Assets written to: NN_model\\assets\n",
      "655/655 [==============================] - 1s 2ms/step - loss: 0.0349 - val_loss: 0.0254\n",
      "Epoch 38/100\n",
      "646/655 [============================>.] - ETA: 0s - loss: 0.0315\n",
      "Epoch 38: val_loss did not improve from 0.02544\n",
      "655/655 [==============================] - 1s 1ms/step - loss: 0.0315 - val_loss: 0.0479\n",
      "Epoch 39/100\n",
      "652/655 [============================>.] - ETA: 0s - loss: 0.0329\n",
      "Epoch 39: val_loss did not improve from 0.02544\n",
      "655/655 [==============================] - 1s 1ms/step - loss: 0.0330 - val_loss: 0.0559\n",
      "Epoch 40/100\n",
      "615/655 [===========================>..] - ETA: 0s - loss: 0.0320\n",
      "Epoch 40: val_loss did not improve from 0.02544\n",
      "655/655 [==============================] - 1s 1ms/step - loss: 0.0318 - val_loss: 0.0326\n",
      "Epoch 41/100\n",
      "648/655 [============================>.] - ETA: 0s - loss: 0.0334\n",
      "Epoch 41: val_loss improved from 0.02544 to 0.02317, saving model to NN_model\n",
      "INFO:tensorflow:Assets written to: NN_model\\assets\n",
      "655/655 [==============================] - 1s 2ms/step - loss: 0.0333 - val_loss: 0.0232\n",
      "Epoch 42/100\n",
      "642/655 [============================>.] - ETA: 0s - loss: 0.0296\n",
      "Epoch 42: val_loss did not improve from 0.02317\n",
      "655/655 [==============================] - 1s 1ms/step - loss: 0.0296 - val_loss: 0.0738\n",
      "Epoch 43/100\n",
      "611/655 [==========================>...] - ETA: 0s - loss: 0.0300\n",
      "Epoch 43: val_loss did not improve from 0.02317\n",
      "655/655 [==============================] - 1s 1ms/step - loss: 0.0299 - val_loss: 0.0301\n",
      "Epoch 44/100\n",
      "621/655 [===========================>..] - ETA: 0s - loss: 0.0299\n",
      "Epoch 44: val_loss did not improve from 0.02317\n",
      "655/655 [==============================] - 1s 1ms/step - loss: 0.0296 - val_loss: 0.0371\n",
      "Epoch 45/100\n",
      "610/655 [==========================>...] - ETA: 0s - loss: 0.0306\n",
      "Epoch 45: val_loss did not improve from 0.02317\n",
      "655/655 [==============================] - 1s 1ms/step - loss: 0.0305 - val_loss: 0.0275\n",
      "Epoch 46/100\n",
      "646/655 [============================>.] - ETA: 0s - loss: 0.0282\n",
      "Epoch 46: val_loss did not improve from 0.02317\n",
      "655/655 [==============================] - 1s 1ms/step - loss: 0.0282 - val_loss: 0.0282\n",
      "Epoch 47/100\n",
      "643/655 [============================>.] - ETA: 0s - loss: 0.0275\n",
      "Epoch 47: val_loss did not improve from 0.02317\n",
      "655/655 [==============================] - 1s 1ms/step - loss: 0.0276 - val_loss: 0.0252\n",
      "Epoch 48/100\n",
      "644/655 [============================>.] - ETA: 0s - loss: 0.0286\n",
      "Epoch 48: val_loss did not improve from 0.02317\n",
      "655/655 [==============================] - 1s 1ms/step - loss: 0.0286 - val_loss: 0.0290\n",
      "Epoch 49/100\n",
      "605/655 [==========================>...] - ETA: 0s - loss: 0.0321\n",
      "Epoch 49: val_loss did not improve from 0.02317\n",
      "655/655 [==============================] - 1s 1ms/step - loss: 0.0314 - val_loss: 0.0290\n",
      "Epoch 50/100\n",
      "617/655 [===========================>..] - ETA: 0s - loss: 0.0259\n",
      "Epoch 50: val_loss improved from 0.02317 to 0.02029, saving model to NN_model\n",
      "INFO:tensorflow:Assets written to: NN_model\\assets\n",
      "655/655 [==============================] - 1s 2ms/step - loss: 0.0257 - val_loss: 0.0203\n",
      "Epoch 51/100\n",
      "614/655 [===========================>..] - ETA: 0s - loss: 0.0279\n",
      "Epoch 51: val_loss did not improve from 0.02029\n",
      "655/655 [==============================] - 1s 1ms/step - loss: 0.0278 - val_loss: 0.0216\n",
      "Epoch 52/100\n",
      "642/655 [============================>.] - ETA: 0s - loss: 0.0264\n",
      "Epoch 52: val_loss did not improve from 0.02029\n",
      "655/655 [==============================] - 1s 1ms/step - loss: 0.0264 - val_loss: 0.0376\n",
      "Epoch 53/100\n",
      "625/655 [===========================>..] - ETA: 0s - loss: 0.0266\n",
      "Epoch 53: val_loss did not improve from 0.02029\n",
      "655/655 [==============================] - 1s 1ms/step - loss: 0.0267 - val_loss: 0.0364\n",
      "Epoch 54/100\n",
      "610/655 [==========================>...] - ETA: 0s - loss: 0.0276\n",
      "Epoch 54: val_loss did not improve from 0.02029\n",
      "655/655 [==============================] - 1s 1ms/step - loss: 0.0273 - val_loss: 0.0208\n",
      "Epoch 55/100\n",
      "641/655 [============================>.] - ETA: 0s - loss: 0.0282\n",
      "Epoch 55: val_loss did not improve from 0.02029\n",
      "655/655 [==============================] - 1s 1ms/step - loss: 0.0281 - val_loss: 0.0222\n",
      "Epoch 56/100\n",
      "630/655 [===========================>..] - ETA: 0s - loss: 0.0277\n",
      "Epoch 56: val_loss did not improve from 0.02029\n",
      "655/655 [==============================] - 1s 1ms/step - loss: 0.0277 - val_loss: 0.0312\n",
      "Epoch 57/100\n",
      "629/655 [===========================>..] - ETA: 0s - loss: 0.0283\n",
      "Epoch 57: val_loss did not improve from 0.02029\n",
      "655/655 [==============================] - 1s 1ms/step - loss: 0.0281 - val_loss: 0.0477\n",
      "Epoch 58/100\n",
      "651/655 [============================>.] - ETA: 0s - loss: 0.0255\n",
      "Epoch 58: val_loss did not improve from 0.02029\n",
      "655/655 [==============================] - 1s 1ms/step - loss: 0.0255 - val_loss: 0.0550\n",
      "Epoch 59/100\n",
      "631/655 [===========================>..] - ETA: 0s - loss: 0.0263\n",
      "Epoch 59: val_loss did not improve from 0.02029\n",
      "655/655 [==============================] - 1s 1ms/step - loss: 0.0264 - val_loss: 0.0272\n",
      "Epoch 60/100\n",
      "627/655 [===========================>..] - ETA: 0s - loss: 0.0251\n",
      "Epoch 60: val_loss improved from 0.02029 to 0.02021, saving model to NN_model\n",
      "INFO:tensorflow:Assets written to: NN_model\\assets\n",
      "655/655 [==============================] - 1s 2ms/step - loss: 0.0251 - val_loss: 0.0202\n",
      "Epoch 61/100\n",
      "644/655 [============================>.] - ETA: 0s - loss: 0.0240\n",
      "Epoch 61: val_loss did not improve from 0.02021\n",
      "655/655 [==============================] - 1s 1ms/step - loss: 0.0241 - val_loss: 0.0206\n",
      "Epoch 62/100\n",
      "621/655 [===========================>..] - ETA: 0s - loss: 0.0245\n",
      "Epoch 62: val_loss did not improve from 0.02021\n",
      "655/655 [==============================] - 1s 1ms/step - loss: 0.0247 - val_loss: 0.0513\n",
      "Epoch 63/100\n",
      "612/655 [===========================>..] - ETA: 0s - loss: 0.0256\n",
      "Epoch 63: val_loss did not improve from 0.02021\n",
      "655/655 [==============================] - 1s 1ms/step - loss: 0.0253 - val_loss: 0.0401\n",
      "Epoch 64/100\n",
      "640/655 [============================>.] - ETA: 0s - loss: 0.0245\n",
      "Epoch 64: val_loss did not improve from 0.02021\n",
      "655/655 [==============================] - 1s 1ms/step - loss: 0.0245 - val_loss: 0.0423\n",
      "Epoch 65/100\n",
      "633/655 [===========================>..] - ETA: 0s - loss: 0.0238\n",
      "Epoch 65: val_loss did not improve from 0.02021\n",
      "655/655 [==============================] - 1s 1ms/step - loss: 0.0240 - val_loss: 0.0506\n",
      "Epoch 66/100\n",
      "629/655 [===========================>..] - ETA: 0s - loss: 0.0240\n",
      "Epoch 66: val_loss did not improve from 0.02021\n",
      "655/655 [==============================] - 1s 1ms/step - loss: 0.0240 - val_loss: 0.0487\n",
      "Epoch 67/100\n",
      "621/655 [===========================>..] - ETA: 0s - loss: 0.0237\n",
      "Epoch 67: val_loss did not improve from 0.02021\n",
      "655/655 [==============================] - 1s 1ms/step - loss: 0.0239 - val_loss: 0.0381\n",
      "Epoch 68/100\n",
      "634/655 [============================>.] - ETA: 0s - loss: 0.0250\n",
      "Epoch 68: val_loss did not improve from 0.02021\n",
      "655/655 [==============================] - 1s 1ms/step - loss: 0.0250 - val_loss: 0.0221\n",
      "Epoch 69/100\n",
      "626/655 [===========================>..] - ETA: 0s - loss: 0.0255\n",
      "Epoch 69: val_loss did not improve from 0.02021\n",
      "655/655 [==============================] - 1s 1ms/step - loss: 0.0254 - val_loss: 0.0211\n",
      "Epoch 70/100\n",
      "618/655 [===========================>..] - ETA: 0s - loss: 0.0254\n",
      "Epoch 70: val_loss improved from 0.02021 to 0.01981, saving model to NN_model\n",
      "INFO:tensorflow:Assets written to: NN_model\\assets\n",
      "655/655 [==============================] - 1s 2ms/step - loss: 0.0251 - val_loss: 0.0198\n",
      "Epoch 71/100\n",
      "622/655 [===========================>..] - ETA: 0s - loss: 0.0236\n",
      "Epoch 71: val_loss did not improve from 0.01981\n",
      "655/655 [==============================] - 1s 1ms/step - loss: 0.0237 - val_loss: 0.0328\n",
      "Epoch 72/100\n",
      "611/655 [==========================>...] - ETA: 0s - loss: 0.0231\n",
      "Epoch 72: val_loss did not improve from 0.01981\n",
      "655/655 [==============================] - 1s 1ms/step - loss: 0.0229 - val_loss: 0.0249\n",
      "Epoch 73/100\n",
      "617/655 [===========================>..] - ETA: 0s - loss: 0.0236\n",
      "Epoch 73: val_loss did not improve from 0.01981\n",
      "655/655 [==============================] - 1s 1ms/step - loss: 0.0235 - val_loss: 0.0384\n",
      "Epoch 74/100\n",
      "617/655 [===========================>..] - ETA: 0s - loss: 0.0247\n",
      "Epoch 74: val_loss did not improve from 0.01981\n",
      "655/655 [==============================] - 1s 1ms/step - loss: 0.0245 - val_loss: 0.0228\n",
      "Epoch 75/100\n",
      "621/655 [===========================>..] - ETA: 0s - loss: 0.0223\n",
      "Epoch 75: val_loss did not improve from 0.01981\n",
      "655/655 [==============================] - 1s 1ms/step - loss: 0.0224 - val_loss: 0.0228\n",
      "Epoch 76/100\n",
      "620/655 [===========================>..] - ETA: 0s - loss: 0.0250\n",
      "Epoch 76: val_loss did not improve from 0.01981\n",
      "655/655 [==============================] - 1s 1ms/step - loss: 0.0248 - val_loss: 0.0306\n",
      "Epoch 77/100\n",
      "635/655 [============================>.] - ETA: 0s - loss: 0.0227\n",
      "Epoch 77: val_loss did not improve from 0.01981\n",
      "655/655 [==============================] - 1s 1ms/step - loss: 0.0226 - val_loss: 0.0200\n",
      "Epoch 78/100\n",
      "609/655 [==========================>...] - ETA: 0s - loss: 0.0221\n",
      "Epoch 78: val_loss improved from 0.01981 to 0.01902, saving model to NN_model\n",
      "INFO:tensorflow:Assets written to: NN_model\\assets\n",
      "655/655 [==============================] - 1s 2ms/step - loss: 0.0225 - val_loss: 0.0190\n",
      "Epoch 79/100\n",
      "654/655 [============================>.] - ETA: 0s - loss: 0.0229\n",
      "Epoch 79: val_loss did not improve from 0.01902\n",
      "655/655 [==============================] - 1s 2ms/step - loss: 0.0229 - val_loss: 0.0286\n",
      "Epoch 80/100\n",
      "642/655 [============================>.] - ETA: 0s - loss: 0.0217\n",
      "Epoch 80: val_loss did not improve from 0.01902\n",
      "655/655 [==============================] - 1s 2ms/step - loss: 0.0217 - val_loss: 0.0472\n",
      "Epoch 81/100\n",
      "647/655 [============================>.] - ETA: 0s - loss: 0.0234\n",
      "Epoch 81: val_loss did not improve from 0.01902\n",
      "655/655 [==============================] - 1s 1ms/step - loss: 0.0234 - val_loss: 0.0277\n",
      "Epoch 82/100\n",
      "649/655 [============================>.] - ETA: 0s - loss: 0.0244\n",
      "Epoch 82: val_loss did not improve from 0.01902\n",
      "655/655 [==============================] - 1s 1ms/step - loss: 0.0245 - val_loss: 0.0445\n",
      "Epoch 83/100\n",
      "623/655 [===========================>..] - ETA: 0s - loss: 0.0231\n",
      "Epoch 83: val_loss did not improve from 0.01902\n",
      "655/655 [==============================] - 1s 1ms/step - loss: 0.0228 - val_loss: 0.0371\n",
      "Epoch 84/100\n",
      "623/655 [===========================>..] - ETA: 0s - loss: 0.0233\n",
      "Epoch 84: val_loss did not improve from 0.01902\n",
      "655/655 [==============================] - 1s 1ms/step - loss: 0.0234 - val_loss: 0.0295\n",
      "Epoch 85/100\n",
      "626/655 [===========================>..] - ETA: 0s - loss: 0.0229\n",
      "Epoch 85: val_loss did not improve from 0.01902\n",
      "655/655 [==============================] - 1s 1ms/step - loss: 0.0227 - val_loss: 0.0274\n",
      "Epoch 86/100\n",
      "641/655 [============================>.] - ETA: 0s - loss: 0.0224\n",
      "Epoch 86: val_loss did not improve from 0.01902\n",
      "655/655 [==============================] - 1s 1ms/step - loss: 0.0224 - val_loss: 0.0289\n",
      "Epoch 87/100\n",
      "653/655 [============================>.] - ETA: 0s - loss: 0.0216\n",
      "Epoch 87: val_loss did not improve from 0.01902\n",
      "655/655 [==============================] - 1s 1ms/step - loss: 0.0216 - val_loss: 0.0197\n",
      "Epoch 88/100\n",
      "628/655 [===========================>..] - ETA: 0s - loss: 0.0213\n",
      "Epoch 88: val_loss improved from 0.01902 to 0.01684, saving model to NN_model\n",
      "INFO:tensorflow:Assets written to: NN_model\\assets\n",
      "655/655 [==============================] - 1s 2ms/step - loss: 0.0211 - val_loss: 0.0168\n",
      "Epoch 89/100\n",
      "608/655 [==========================>...] - ETA: 0s - loss: 0.0231\n",
      "Epoch 89: val_loss did not improve from 0.01684\n",
      "655/655 [==============================] - 1s 1ms/step - loss: 0.0228 - val_loss: 0.0237\n",
      "Epoch 90/100\n",
      "648/655 [============================>.] - ETA: 0s - loss: 0.0223\n",
      "Epoch 90: val_loss did not improve from 0.01684\n",
      "655/655 [==============================] - 1s 1ms/step - loss: 0.0224 - val_loss: 0.0283\n",
      "Epoch 91/100\n",
      "615/655 [===========================>..] - ETA: 0s - loss: 0.0216\n",
      "Epoch 91: val_loss did not improve from 0.01684\n",
      "655/655 [==============================] - 1s 1ms/step - loss: 0.0217 - val_loss: 0.0512\n",
      "Epoch 92/100\n",
      "627/655 [===========================>..] - ETA: 0s - loss: 0.0209\n",
      "Epoch 92: val_loss did not improve from 0.01684\n",
      "655/655 [==============================] - 1s 1ms/step - loss: 0.0211 - val_loss: 0.0202\n",
      "Epoch 93/100\n",
      "607/655 [==========================>...] - ETA: 0s - loss: 0.0217\n",
      "Epoch 93: val_loss did not improve from 0.01684\n",
      "655/655 [==============================] - 1s 1ms/step - loss: 0.0217 - val_loss: 0.0234\n",
      "Epoch 94/100\n",
      "644/655 [============================>.] - ETA: 0s - loss: 0.0212\n",
      "Epoch 94: val_loss did not improve from 0.01684\n",
      "655/655 [==============================] - 1s 1ms/step - loss: 0.0212 - val_loss: 0.0189\n",
      "Epoch 95/100\n",
      "640/655 [============================>.] - ETA: 0s - loss: 0.0218\n",
      "Epoch 95: val_loss did not improve from 0.01684\n",
      "655/655 [==============================] - 1s 1ms/step - loss: 0.0218 - val_loss: 0.0200\n",
      "Epoch 96/100\n",
      "604/655 [==========================>...] - ETA: 0s - loss: 0.0214\n",
      "Epoch 96: val_loss did not improve from 0.01684\n",
      "655/655 [==============================] - 1s 1ms/step - loss: 0.0213 - val_loss: 0.0213\n",
      "Epoch 97/100\n",
      "617/655 [===========================>..] - ETA: 0s - loss: 0.0215\n",
      "Epoch 97: val_loss did not improve from 0.01684\n",
      "655/655 [==============================] - 1s 1ms/step - loss: 0.0215 - val_loss: 0.0199\n",
      "Epoch 98/100\n",
      "636/655 [============================>.] - ETA: 0s - loss: 0.0206\n",
      "Epoch 98: val_loss did not improve from 0.01684\n",
      "655/655 [==============================] - 1s 1ms/step - loss: 0.0205 - val_loss: 0.0241\n",
      "Epoch 99/100\n",
      "618/655 [===========================>..] - ETA: 0s - loss: 0.0214\n",
      "Epoch 99: val_loss did not improve from 0.01684\n",
      "655/655 [==============================] - 1s 1ms/step - loss: 0.0214 - val_loss: 0.0229\n",
      "Epoch 100/100\n",
      "637/655 [============================>.] - ETA: 0s - loss: 0.0212\n",
      "Epoch 100: val_loss did not improve from 0.01684\n",
      "655/655 [==============================] - 1s 1ms/step - loss: 0.0211 - val_loss: 0.0193\n"
     ]
    }
   ],
   "source": [
    "optimizer = CalibrateSimulation(OptimizerType.OR_TOOLS)\n",
    "optimizer.train_model(X_training, Y_training, X_validation, Y_validation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 done\n",
      "1 done\n",
      "2 done\n",
      "3 done\n",
      "4 done\n",
      "5 done\n",
      "6 done\n",
      "7 done\n",
      "8 done\n",
      "9 done\n",
      "10 done\n",
      "11 done\n",
      "12 done\n",
      "13 done\n",
      "14 done\n",
      "15 done\n",
      "16 done\n",
      "17 done\n",
      "18 done\n",
      "19 done\n",
      "20 done\n",
      "21 done\n",
      "22 done\n",
      "23 done\n",
      "24 done\n",
      "25 done\n",
      "26 done\n",
      "27 done\n",
      "28 done\n",
      "29 done\n",
      "30 done\n",
      "31 done\n",
      "32 done\n",
      "33 done\n",
      "34 done\n",
      "35 done\n",
      "36 done\n",
      "37 done\n",
      "38 done\n",
      "39 done\n",
      "40 done\n",
      "41 done\n",
      "42 done\n",
      "43 done\n",
      "44 done\n",
      "45 done\n",
      "46 done\n",
      "47 done\n",
      "48 done\n",
      "49 done\n",
      "50 done\n",
      "51 done\n",
      "52 done\n",
      "53 done\n",
      "54 done\n",
      "55 done\n",
      "56 done\n",
      "57 done\n",
      "58 done\n",
      "59 done\n",
      "60 done\n",
      "61 done\n",
      "62 done\n",
      "63 done\n",
      "64 done\n",
      "65 done\n",
      "66 done\n",
      "67 done\n",
      "68 done\n",
      "69 done\n",
      "70 done\n",
      "71 done\n",
      "72 done\n",
      "73 done\n",
      "74 done\n",
      "75 done\n",
      "76 done\n",
      "77 done\n",
      "78 done\n",
      "79 done\n",
      "80 done\n",
      "81 done\n",
      "82 done\n",
      "83 done\n",
      "84 done\n",
      "85 done\n",
      "86 done\n",
      "87 done\n",
      "88 done\n",
      "89 done\n",
      "90 done\n",
      "91 done\n",
      "92 done\n",
      "93 done\n",
      "94 done\n",
      "95 done\n",
      "96 done\n",
      "97 done\n",
      "98 done\n",
      "99 done\n"
     ]
    }
   ],
   "source": [
    "sample_arrays = np.array(Target_Sample)\n",
    "lower_bound = [1, 0.5, 5, 1, 0.4, 390]\n",
    "upper_bound = [2.5, 1, 10, 5, 0.8,760]\n",
    "\n",
    "results_param = {}\n",
    "results_output = {}\n",
    "for i in range(100):\n",
    "    result = optimizer.solve_optimization(sample_arrays[i],lower_bound,upper_bound)\n",
    "    results_param[i] = result[0]\n",
    "    results_output[i] = result[1]\n",
    "    print(f'{i} done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [1.9017864096281392, 0.5, 4.999999999999999, 1.0, 0.4, 593.1479655581693],\n",
       " 1: [1.0459946605063444,\n",
       "  1.0,\n",
       "  10.0,\n",
       "  4.29313000946394,\n",
       "  0.799999999999999,\n",
       "  760.0],\n",
       " 2: [1.2126412929948818,\n",
       "  1.0,\n",
       "  10.0,\n",
       "  3.0886420595672717,\n",
       "  0.4188223665726387,\n",
       "  619.0464880799664],\n",
       " 3: [1.3633434320756341,\n",
       "  0.5,\n",
       "  5.000000000000019,\n",
       "  1.0,\n",
       "  0.4444223853961062,\n",
       "  760.0],\n",
       " 4: [1.5865158420909167,\n",
       "  0.6666362128971505,\n",
       "  10.0,\n",
       "  3.6539687808126065,\n",
       "  0.4,\n",
       "  760.0],\n",
       " 5: [1.1628844989832543,\n",
       "  0.7491694691796429,\n",
       "  8.650414648825722,\n",
       "  3.90279154629549,\n",
       "  0.8,\n",
       "  760.0],\n",
       " 6: [2.3612115214820024, 0.5, 4.999999999999999, 1.0, 0.4, 499.07415720124254],\n",
       " 7: [1.0791888255906517, 1.0, 10.0, 4.220347182781768, 0.8, 760.0],\n",
       " 8: [1.5368367393720535,\n",
       "  1.0,\n",
       "  5.000000000000001,\n",
       "  0.9999999999999997,\n",
       "  0.5305030943817475,\n",
       "  404.60389274965627],\n",
       " 9: [2.231117964517724,\n",
       "  1.0,\n",
       "  5.919179539874616,\n",
       "  4.0276778839695275,\n",
       "  0.39999999999999997,\n",
       "  390.00000000000006],\n",
       " 10: [1.9533838501536465, 0.5, 5.0, 3.644548603796746, 0.4, 760.0],\n",
       " 11: [1.6893838903246448,\n",
       "  1.0,\n",
       "  10.0,\n",
       "  3.722836453808589,\n",
       "  0.45086830202272665,\n",
       "  630.4032601398309],\n",
       " 12: [1.635599113089636, 0.6218838376805459, 5.0, 1.0, 0.8, 667.9333151070034],\n",
       " 13: [1.615326477797683,\n",
       "  0.8980389124968212,\n",
       "  5.000000000000001,\n",
       "  4.097505883235968,\n",
       "  0.4,\n",
       "  760.0],\n",
       " 14: [1.788356973391854, 1.0, 10.0, 2.913752840581815, 0.8, 760.0],\n",
       " 15: [1.4530360677503784,\n",
       "  0.5686556741021843,\n",
       "  5.0,\n",
       "  1.0,\n",
       "  0.8,\n",
       "  683.9602469297959],\n",
       " 16: [2.3533835402703964, 1.0, 5.0, 1.0, 0.4394575585547749, 648.594381345388],\n",
       " 17: [1.7191673547015796,\n",
       "  1.0,\n",
       "  4.99999999999996,\n",
       "  4.525390773079463,\n",
       "  0.8,\n",
       "  760.0],\n",
       " 18: [1.345160975474895, 0.5, 5.0, 1.0, 0.6303524725760495, 690.9943349546],\n",
       " 19: [2.2573824881632034,\n",
       "  0.7800494963374481,\n",
       "  5.0,\n",
       "  4.537502075812079,\n",
       "  0.4,\n",
       "  390.0],\n",
       " 20: [1.9554774111547988,\n",
       "  0.49999999999999967,\n",
       "  5.0,\n",
       "  4.53958376194218,\n",
       "  0.4,\n",
       "  391.21431731568543],\n",
       " 21: [1.1065650306934396,\n",
       "  1.0,\n",
       "  6.055279481367276,\n",
       "  4.134914465352571,\n",
       "  0.8,\n",
       "  760.0],\n",
       " 22: [1.1430695834864448,\n",
       "  1.0,\n",
       "  5.0,\n",
       "  4.580347674705268,\n",
       "  0.6329096677292463,\n",
       "  390.00000000000006],\n",
       " 23: [1.542145107829489, 0.5, 10.0, 2.786289761575841, 0.4, 434.118803291429],\n",
       " 24: [1.1630446326114496,\n",
       "  0.9654948761073997,\n",
       "  5.0,\n",
       "  4.164604596256424,\n",
       "  0.4000000000000048,\n",
       "  390.0],\n",
       " 25: [2.4196561229840547,\n",
       "  0.5,\n",
       "  5.0,\n",
       "  3.6120798987330884,\n",
       "  0.4,\n",
       "  759.9999999999999],\n",
       " 26: [1.0610967813817156, 1.0, 10.000000000000004, 5.0, 0.8, 760.0],\n",
       " 27: [1.6566538276523515,\n",
       "  0.7622097605396018,\n",
       "  5.000000000000039,\n",
       "  3.4659555228742116,\n",
       "  0.4,\n",
       "  760.0],\n",
       " 28: [1.5130936666503023,\n",
       "  0.6840770276259509,\n",
       "  10.0,\n",
       "  1.0000000000000004,\n",
       "  0.4,\n",
       "  675.1764362766999],\n",
       " 29: [2.456776396275274,\n",
       "  0.5,\n",
       "  5.0400222114767175,\n",
       "  4.658387852897466,\n",
       "  0.39999999999999997,\n",
       "  390.0],\n",
       " 30: [1.6702900695265879,\n",
       "  1.0,\n",
       "  5.0,\n",
       "  5.0,\n",
       "  0.5112958066796973,\n",
       "  468.6882952804528],\n",
       " 31: [1.604537944416451, 0.8508675046134002, 10.0, 1.0, 0.8, 587.272825321068],\n",
       " 32: [1.3398593602571593,\n",
       "  0.5216612967738721,\n",
       "  9.876275284932579,\n",
       "  3.570502954885744,\n",
       "  0.4,\n",
       "  760.0],\n",
       " 33: [1.7173125506572422, 1.0, 5.0, 4.999999999999996, 0.4, 390.0],\n",
       " 34: [1.5054800632439294,\n",
       "  0.7146457523574031,\n",
       "  9.999999999999996,\n",
       "  1.9117609230257078,\n",
       "  0.4,\n",
       "  390.0],\n",
       " 35: [2.488186443285813,\n",
       "  0.5,\n",
       "  5.438205054567935,\n",
       "  1.004579507882014,\n",
       "  0.4000000000000001,\n",
       "  760.0],\n",
       " 36: [2.206592824185784,\n",
       "  0.49999999999999994,\n",
       "  5.0,\n",
       "  3.863652744419256,\n",
       "  0.4,\n",
       "  760.0],\n",
       " 37: [1.62639326875047,\n",
       "  0.7657991694044164,\n",
       "  5.000000000000001,\n",
       "  3.8075624854182064,\n",
       "  0.4,\n",
       "  760.0],\n",
       " 38: [2.4977038326345844, 0.5, 5.0, 1.0, 0.7900326761794685, 760.0],\n",
       " 39: [1.962015422764834, 0.5, 6.441197812490473, 1.0, 0.8, 760.0],\n",
       " 40: [1.0006414812570474,\n",
       "  1.0,\n",
       "  9.999999999999998,\n",
       "  4.544331591424423,\n",
       "  0.8000000000000014,\n",
       "  760.0],\n",
       " 41: [1.8942390870773471,\n",
       "  0.5,\n",
       "  5.0,\n",
       "  3.2697724728927455,\n",
       "  0.4,\n",
       "  673.0121416286596],\n",
       " 42: [1.5384523732129676,\n",
       "  0.7774311940664435,\n",
       "  10.0,\n",
       "  1.0,\n",
       "  0.4,\n",
       "  508.0735973711156],\n",
       " 43: [1.7235721315236747, 0.9739974386855141, 5.0, 1.0, 0.8, 760.0],\n",
       " 44: [2.3234307050180227,\n",
       "  0.5,\n",
       "  6.3513318558099305,\n",
       "  3.3016479120413673,\n",
       "  0.4,\n",
       "  760.0000000000001],\n",
       " 45: [2.3143549477797323,\n",
       "  1.0,\n",
       "  10.0,\n",
       "  4.302680230310494,\n",
       "  0.5389635650097783,\n",
       "  390.0],\n",
       " 46: [1.8429451095314908,\n",
       "  1.0,\n",
       "  8.510586156165528,\n",
       "  4.544040475033463,\n",
       "  0.8,\n",
       "  390.0],\n",
       " 47: [1.2070285207260236,\n",
       "  0.7683493187681827,\n",
       "  5.0,\n",
       "  4.441386839694939,\n",
       "  0.4,\n",
       "  390.00000000000017],\n",
       " 48: [2.140426619875291, 0.5, 5.0, 3.3609705456898737, 0.4, 390.0000000000001],\n",
       " 49: [1.2549992300377533,\n",
       "  0.6811342789459333,\n",
       "  5.0,\n",
       "  0.9999999999999939,\n",
       "  0.8,\n",
       "  553.9167229499118],\n",
       " 50: [1.6158418531747145,\n",
       "  0.5308683883747692,\n",
       "  10.000000000000002,\n",
       "  3.474287048530681,\n",
       "  0.40000000000000496,\n",
       "  760.0],\n",
       " 51: [1.3811148532380133,\n",
       "  0.7530194065549872,\n",
       "  10.0,\n",
       "  1.0,\n",
       "  0.8,\n",
       "  424.5365634346771],\n",
       " 52: [1.3102547550268604,\n",
       "  0.5,\n",
       "  5.0,\n",
       "  2.3669545457641035,\n",
       "  0.40000000000000485,\n",
       "  743.4045553299582],\n",
       " 53: [1.4848223251288162,\n",
       "  0.523746557861783,\n",
       "  5.0,\n",
       "  0.9999999999999998,\n",
       "  0.7999999999999999,\n",
       "  505.9844987606939],\n",
       " 54: [1.2474598406687287, 0.5, 5.0, 2.380703150646578, 0.8, 760.0],\n",
       " 55: [1.7674921705686515, 1.0, 5.0, 2.175274176251257, 0.8, 760.0],\n",
       " 56: [2.190497709110846,\n",
       "  0.5256942950023984,\n",
       "  10.00000000000001,\n",
       "  1.0,\n",
       "  0.8000000000000003,\n",
       "  759.9999999999995],\n",
       " 57: [1.1939534747278302,\n",
       "  1.0000000000000004,\n",
       "  10.0,\n",
       "  2.902029347820463,\n",
       "  0.7828960911709274,\n",
       "  667.8781126251814],\n",
       " 58: [2.4197858895760245,\n",
       "  0.5,\n",
       "  8.596851751027346,\n",
       "  2.1357241024466393,\n",
       "  0.4,\n",
       "  760.0],\n",
       " 59: [2.4246708205998253,\n",
       "  0.5224820845780854,\n",
       "  10.0,\n",
       "  3.8112012946397176,\n",
       "  0.4,\n",
       "  760.0],\n",
       " 60: [1.5041769613850517,\n",
       "  0.6247122163322425,\n",
       "  5.0,\n",
       "  1.0000000000000002,\n",
       "  0.8000000000000002,\n",
       "  598.6305654158439],\n",
       " 61: [2.2919340594512123,\n",
       "  1.0,\n",
       "  9.93995503937073,\n",
       "  4.926021635213629,\n",
       "  0.515734837214053,\n",
       "  390.0],\n",
       " 62: [1.7132819799016377,\n",
       "  1.0,\n",
       "  8.118074853770148,\n",
       "  5.0,\n",
       "  0.5379552981494742,\n",
       "  390.0000000000003],\n",
       " 63: [2.42371167844685,\n",
       "  0.515147350650121,\n",
       "  8.617471173075945,\n",
       "  1.0,\n",
       "  0.8,\n",
       "  540.8777256752929],\n",
       " 64: [1.070382617331259,\n",
       "  1.0,\n",
       "  10.000000000000002,\n",
       "  4.323496698799229,\n",
       "  0.8,\n",
       "  760.0],\n",
       " 65: [2.252824511983173,\n",
       "  0.5,\n",
       "  9.715114101617932,\n",
       "  1.0000000000000002,\n",
       "  0.4,\n",
       "  534.5610710591721],\n",
       " 66: [1.7681022442249037,\n",
       "  1.0,\n",
       "  10.0,\n",
       "  3.6506866026382125,\n",
       "  0.4995666270245385,\n",
       "  760.0],\n",
       " 67: [2.5, 0.5307249625110582, 5.0, 1.0, 0.4, 390.0],\n",
       " 68: [1.801262198365306, 0.5, 5.0, 4.024130619462861, 0.4, 390.0],\n",
       " 69: [2.0625867498921946,\n",
       "  0.5,\n",
       "  8.277321950660056,\n",
       "  3.647180757370452,\n",
       "  0.4,\n",
       "  760.0],\n",
       " 70: [1.329386407645647,\n",
       "  0.5,\n",
       "  10.0,\n",
       "  1.0,\n",
       "  0.5265327875187243,\n",
       "  676.0247537326783],\n",
       " 71: [1.2753988234488487, 0.58703486397088, 10.0, 1.0, 0.8, 652.1733214722076],\n",
       " 72: [1.0872132749276275,\n",
       "  1.0000000000000002,\n",
       "  10.0,\n",
       "  4.252064092216054,\n",
       "  0.8000000000000002,\n",
       "  760.0],\n",
       " 73: [1.0632000229367529,\n",
       "  1.0,\n",
       "  10.000000000000005,\n",
       "  4.38473943755516,\n",
       "  0.8,\n",
       "  760.0],\n",
       " 74: [2.2293728912455073,\n",
       "  0.6434625201970462,\n",
       "  9.999999999999998,\n",
       "  1.0,\n",
       "  0.4,\n",
       "  513.1397685162552],\n",
       " 75: [1.2071940316284042,\n",
       "  0.7586522218760416,\n",
       "  10.0,\n",
       "  1.0,\n",
       "  0.7999999999999988,\n",
       "  760.0],\n",
       " 76: [2.122987990119319,\n",
       "  1.0,\n",
       "  10.0,\n",
       "  4.82549679995694,\n",
       "  0.41103994745146144,\n",
       "  390.0],\n",
       " 77: [1.3099162198889522,\n",
       "  1.0,\n",
       "  4.999999999999999,\n",
       "  1.0,\n",
       "  0.7934949894657038,\n",
       "  551.1216415665168],\n",
       " 78: [2.1494747902435614,\n",
       "  0.5,\n",
       "  8.693374167541931,\n",
       "  3.373319434617341,\n",
       "  0.4,\n",
       "  581.7755464143687],\n",
       " 79: [1.5107780892843596,\n",
       "  0.9476216240396699,\n",
       "  9.999999999999995,\n",
       "  3.8509128584288352,\n",
       "  0.4000000000000001,\n",
       "  760.0],\n",
       " 80: [1.6324992753862146,\n",
       "  0.8675950918711537,\n",
       "  9.999999999999998,\n",
       "  5.0,\n",
       "  0.4,\n",
       "  391.4714908424993],\n",
       " 81: [1.9655210435039643,\n",
       "  0.49999999999999806,\n",
       "  5.000000000000073,\n",
       "  4.406828681648323,\n",
       "  0.4,\n",
       "  390.00000000000017],\n",
       " 82: [2.0118343051589878, 0.5, 5.0, 4.589008851092234, 0.4, 390.0],\n",
       " 83: [2.143305417412141,\n",
       "  0.5,\n",
       "  8.000386844346448,\n",
       "  3.276883469621643,\n",
       "  0.4,\n",
       "  680.115739120289],\n",
       " 84: [2.0761453426896606,\n",
       "  0.8451698921818699,\n",
       "  9.825392748591442,\n",
       "  3.8762605365128024,\n",
       "  0.4000000000000001,\n",
       "  389.99999999999994],\n",
       " 85: [1.6436189173253977,\n",
       "  0.8389232067878538,\n",
       "  5.000000000000001,\n",
       "  1.0,\n",
       "  0.4,\n",
       "  581.511408421676],\n",
       " 86: [2.1319560745776767,\n",
       "  1.0,\n",
       "  5.0,\n",
       "  4.769367043503555,\n",
       "  0.6250298650604389,\n",
       "  390.00000000000006],\n",
       " 87: [1.968501672474052, 0.5, 5.0, 1.0, 0.4000000000000001, 497.371035454197],\n",
       " 88: [1.3354803589221085, 0.5272157967527662, 5.0, 1.0, 0.4, 760.0],\n",
       " 89: [2.093060266525217,\n",
       "  0.5000000000000001,\n",
       "  9.999999999999996,\n",
       "  4.3487552141063235,\n",
       "  0.4,\n",
       "  414.69085446927835],\n",
       " 90: [2.426074387662416,\n",
       "  0.5,\n",
       "  8.417489305919263,\n",
       "  4.585385519582049,\n",
       "  0.4,\n",
       "  396.4199499961535],\n",
       " 91: [2.380029506524348,\n",
       "  0.5,\n",
       "  8.305993720264068,\n",
       "  1.0000000000000056,\n",
       "  0.4,\n",
       "  537.9659941314491],\n",
       " 92: [1.4279758510072382, 0.5, 5.0, 1.0, 0.4, 704.6751349963025],\n",
       " 93: [1.870898901897408, 0.5, 5.0, 3.576667048947533, 0.4, 625.3524588368159],\n",
       " 94: [1.3604746716430018,\n",
       "  0.5679134596138193,\n",
       "  5.0,\n",
       "  4.115745021623548,\n",
       "  0.4731621420058801,\n",
       "  390.0],\n",
       " 95: [1.1535957015116016,\n",
       "  1.0,\n",
       "  5.0,\n",
       "  4.586514107310387,\n",
       "  0.4255828073634539,\n",
       "  390.0],\n",
       " 96: [1.7588242312889355,\n",
       "  1.0,\n",
       "  5.0,\n",
       "  2.280080807763061,\n",
       "  0.7999999999999999,\n",
       "  742.3880359019566],\n",
       " 97: [1.605835744032089,\n",
       "  1.0,\n",
       "  5.0,\n",
       "  2.3869588027074102,\n",
       "  0.7999999999999988,\n",
       "  421.95839156215396],\n",
       " 98: [1.0,\n",
       "  1.0,\n",
       "  9.999999999999998,\n",
       "  4.248153962409802,\n",
       "  0.7999999999999985,\n",
       "  759.9839628694335],\n",
       " 99: [1.2921085141501572,\n",
       "  0.586401902859648,\n",
       "  5.0,\n",
       "  2.399877854663376,\n",
       "  0.4,\n",
       "  738.6210245445453]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [0.7295953944730242,\n",
       "  0.7202500368175964,\n",
       "  0.6622022237694446,\n",
       "  0.6816473657277933,\n",
       "  0.6512359192886076,\n",
       "  0.5741676409088042,\n",
       "  0.6019786910197868,\n",
       "  0.5456529480795654,\n",
       "  0.5780326640736557],\n",
       " 1: [0.0980250272076269,\n",
       "  0.0762324454485575,\n",
       "  0.038759590677005594,\n",
       "  0.05767681129396809,\n",
       "  0.046456458901943526,\n",
       "  0.032617467939930186,\n",
       "  0.0468036529680365,\n",
       "  0.048651547522152484,\n",
       "  0.05120587725984501],\n",
       " 2: [0.2309789453651945,\n",
       "  0.21327958858896964,\n",
       "  0.15856903920067747,\n",
       "  0.17635429465862362,\n",
       "  0.1579816164758571,\n",
       "  0.12676056338028172,\n",
       "  0.14510949486157787,\n",
       "  0.1331990880714687,\n",
       "  0.14609011915918838],\n",
       " 3: [0.37601020074621677,\n",
       "  0.36130950582083815,\n",
       "  0.28871068185593973,\n",
       "  0.3103934205196494,\n",
       "  0.28267869850330074,\n",
       "  0.22154130526863633,\n",
       "  0.2448730245626209,\n",
       "  0.2219297091311608,\n",
       "  0.23676736769662024],\n",
       " 4: [0.5542105676985166,\n",
       "  0.5415665853956831,\n",
       "  0.4751090748589672,\n",
       "  0.49639936750651853,\n",
       "  0.4559041638903898,\n",
       "  0.4062258766139171,\n",
       "  0.4322678843226788,\n",
       "  0.3916141377218428,\n",
       "  0.42026427163198965],\n",
       " 5: [0.19626634814409105,\n",
       "  0.1775038606164511,\n",
       "  0.12766869732724162,\n",
       "  0.14685478975118146,\n",
       "  0.13081264885415114,\n",
       "  0.1000021997741684,\n",
       "  0.11719939117199393,\n",
       "  0.110168347312422,\n",
       "  0.11902795553414418],\n",
       " 6: [0.9161567287898333,\n",
       "  0.9110578537463572,\n",
       "  0.8900931885420179,\n",
       "  0.9013908064928279,\n",
       "  0.8970575746158539,\n",
       "  0.7998064740612927,\n",
       "  0.8257229832572297,\n",
       "  0.7554054706032243,\n",
       "  0.7921497093851805],\n",
       " 7: [0.12049827459143891,\n",
       "  0.0996604012649709,\n",
       "  0.05977095770552628,\n",
       "  0.07865159650088525,\n",
       "  0.06842462236225073,\n",
       "  0.046583651541177835,\n",
       "  0.06126331811263309,\n",
       "  0.0615159776050043,\n",
       "  0.06542620437418291],\n",
       " 8: [0.5378610701727331,\n",
       "  0.521897352740076,\n",
       "  0.46057228928416705,\n",
       "  0.48238490084502983,\n",
       "  0.43904929384349145,\n",
       "  0.4078547371156053,\n",
       "  0.4337899543378995,\n",
       "  0.38959326120324694,\n",
       "  0.4160027907108289],\n",
       " 9: [0.886811180881751,\n",
       "  0.8802984706398849,\n",
       "  0.8593278692032658,\n",
       "  0.8704932649353502,\n",
       "  0.8510740837118992,\n",
       "  0.7859813408784769,\n",
       "  0.8118959039142462,\n",
       "  0.7422222222222223,\n",
       "  0.7785203903192216],\n",
       " 10: [0.7622771459274003,\n",
       "  0.7527474498760094,\n",
       "  0.7022892089227293,\n",
       "  0.7208662291235978,\n",
       "  0.682994481675405,\n",
       "  0.6289931020654069,\n",
       "  0.6565841578668355,\n",
       "  0.5981869842275986,\n",
       "  0.633923950570436],\n",
       " 11: [0.6251639007211732,\n",
       "  0.6134439438858477,\n",
       "  0.551601825580548,\n",
       "  0.5728655371532853,\n",
       "  0.5335294522634001,\n",
       "  0.4761548944995283,\n",
       "  0.5026636225266361,\n",
       "  0.45560157954598135,\n",
       "  0.48701730500548496],\n",
       " 12: [0.5981555664452035,\n",
       "  0.5861016963716428,\n",
       "  0.5152718911462464,\n",
       "  0.5381301338532185,\n",
       "  0.5006458855283132,\n",
       "  0.43313974588972387,\n",
       "  0.4604261796042617,\n",
       "  0.4153581169442568,\n",
       "  0.443155349157634],\n",
       " 13: [0.5890033473873054,\n",
       "  0.5744762684434236,\n",
       "  0.5161378428865884,\n",
       "  0.5374627150267683,\n",
       "  0.48785186758354787,\n",
       "  0.46610634050193106,\n",
       "  0.49258056331704353,\n",
       "  0.4466974443626399,\n",
       "  0.4768439108061748],\n",
       " 14: [0.6858393132496406,\n",
       "  0.6762697612795062,\n",
       "  0.6140943462188025,\n",
       "  0.6354933651418312,\n",
       "  0.6016495299957262,\n",
       "  0.5221327967806842,\n",
       "  0.549538980588369,\n",
       "  0.49924811053554263,\n",
       "  0.5318890918627405],\n",
       " 15: [0.4447067766201534,\n",
       "  0.4306415269408151,\n",
       "  0.3617826763074826,\n",
       "  0.38334240909817485,\n",
       "  0.35448788337419473,\n",
       "  0.2949870595091953,\n",
       "  0.31902660828597007,\n",
       "  0.2881653354071893,\n",
       "  0.30879210647921757],\n",
       " 16: [0.9233235454851569,\n",
       "  0.919147344900827,\n",
       "  0.8949870549102994,\n",
       "  0.9074621713412513,\n",
       "  0.9048360314272611,\n",
       "  0.7938255819817598,\n",
       "  0.820107467803994,\n",
       "  0.7511111111111111,\n",
       "  0.7876179667901015],\n",
       " 17: [0.6580685299289228,\n",
       "  0.6445545565195042,\n",
       "  0.5926161896625773,\n",
       "  0.6137218665897227,\n",
       "  0.5657665500144116,\n",
       "  0.5372233400402415,\n",
       "  0.5640733307973625,\n",
       "  0.512863293041304,\n",
       "  0.5455541830880679],\n",
       " 18: [0.3609017487273127,\n",
       "  0.345794653267918,\n",
       "  0.2749375766209138,\n",
       "  0.29613689229491225,\n",
       "  0.2698914655591898,\n",
       "  0.21531714090988477,\n",
       "  0.23797269131133708,\n",
       "  0.21460604161779312,\n",
       "  0.2316227928244842],\n",
       " 19: [0.9086047574867022,\n",
       "  0.8994269046250096,\n",
       "  0.8889457685534021,\n",
       "  0.8983754346646764,\n",
       "  0.8653780851682046,\n",
       "  0.8490945674044262,\n",
       "  0.8753164409736931,\n",
       "  0.8001470264869562,\n",
       "  0.8394081489976095],\n",
       " 20: [0.8025184922714407,\n",
       "  0.7869325405942406,\n",
       "  0.7559530060303216,\n",
       "  0.7726135178631418,\n",
       "  0.7082231088156667,\n",
       "  0.7397264096233814,\n",
       "  0.7680547731199967,\n",
       "  0.6967157642258557,\n",
       "  0.7380655613928504],\n",
       " 21: [0.14559011374394684,\n",
       "  0.1254111962560241,\n",
       "  0.0821961556231028,\n",
       "  0.10042212078303508,\n",
       "  0.08732815769175563,\n",
       "  0.0675202167155942,\n",
       "  0.08313822492476657,\n",
       "  0.08034997313316031,\n",
       "  0.08628259381759881],\n",
       " 22: [0.2287391997970063,\n",
       "  0.20461245190892016,\n",
       "  0.1667860866693642,\n",
       "  0.18441199905384748,\n",
       "  0.1422049726767666,\n",
       "  0.188174317934683,\n",
       "  0.2066033426125327,\n",
       "  0.18731818663342814,\n",
       "  0.2025615980705225],\n",
       " 23: [0.5520333614094348,\n",
       "  0.5371425280406602,\n",
       "  0.46862957138241496,\n",
       "  0.4908880158120909,\n",
       "  0.4410670958861575,\n",
       "  0.41043977546588,\n",
       "  0.437214611872146,\n",
       "  0.3923085044909313,\n",
       "  0.4221417627155912],\n",
       " 24: [0.24585738740446392,\n",
       "  0.2230779670150205,\n",
       "  0.17785613096285913,\n",
       "  0.1963086997248151,\n",
       "  0.1549987572392273,\n",
       "  0.18611670020120724,\n",
       "  0.2053480615501856,\n",
       "  0.18531277542400676,\n",
       "  0.20100587015711474],\n",
       " 25: [0.942218259612994,\n",
       "  0.9383233860769066,\n",
       "  0.9216688584992228,\n",
       "  0.9322116472466422,\n",
       "  0.9260759008812981,\n",
       "  0.8353716670863536,\n",
       "  0.8608021918972371,\n",
       "  0.7911807357728365,\n",
       "  0.8301886792452832],\n",
       " 26: [0.1339873894299545,\n",
       "  0.1089901545350956,\n",
       "  0.08013958852724831,\n",
       "  0.09888306112161899,\n",
       "  0.07115639765077371,\n",
       "  0.10449013207064252,\n",
       "  0.11971244231788102,\n",
       "  0.11436051773904947,\n",
       "  0.12170446338512612],\n",
       " 27: [0.6097895152398161,\n",
       "  0.5983262982292875,\n",
       "  0.5293327193670128,\n",
       "  0.5518800822311611,\n",
       "  0.5112834509774634,\n",
       "  0.44989229464222175,\n",
       "  0.4768607396221639,\n",
       "  0.43208390557639637,\n",
       "  0.46154086256956206],\n",
       " 28: [0.4970072372499473,\n",
       "  0.48383635180051054,\n",
       "  0.4121722787273055,\n",
       "  0.43359713467612016,\n",
       "  0.3996263549301429,\n",
       "  0.3392942640510296,\n",
       "  0.3649343874946097,\n",
       "  0.32860160308965874,\n",
       "  0.35334476843910806],\n",
       " 29: [0.9840732707451787,\n",
       "  0.9774987951790223,\n",
       "  0.9849907912399057,\n",
       "  0.9908597001564248,\n",
       "  0.9660680254336156,\n",
       "  0.9537223340040245,\n",
       "  0.9806214860591267,\n",
       "  0.8987301794547276,\n",
       "  0.941367217497023],\n",
       " 30: [0.6754881757947184,\n",
       "  0.6564136053922882,\n",
       "  0.6128199033629951,\n",
       "  0.6338932862369463,\n",
       "  0.5584710191688557,\n",
       "  0.6034006832950641,\n",
       "  0.6316590563165905,\n",
       "  0.5704014101082863,\n",
       "  0.6081459707950263],\n",
       " 31: [0.5657212048498662,\n",
       "  0.5523338968401831,\n",
       "  0.48953578690292526,\n",
       "  0.5106335211623761,\n",
       "  0.47473578205826855,\n",
       "  0.4197940972742132,\n",
       "  0.445966514459665,\n",
       "  0.4024375398251873,\n",
       "  0.4301329569176433],\n",
       " 32: [0.3618634354658681,\n",
       "  0.3458284727526525,\n",
       "  0.28188598478136623,\n",
       "  0.30178152218452875,\n",
       "  0.2677000057178664,\n",
       "  0.23742454728370213,\n",
       "  0.25986883584288223,\n",
       "  0.23566375333942688,\n",
       "  0.2557604227193559],\n",
       " 33: [0.7264364775438411,\n",
       "  0.707153167265856,\n",
       "  0.6627173096900845,\n",
       "  0.6843792703732636,\n",
       "  0.6012638822121821,\n",
       "  0.6549401183459707,\n",
       "  0.6845509893455098,\n",
       "  0.6170629603311064,\n",
       "  0.6579959440605934],\n",
       " 34: [0.5202900213894674,\n",
       "  0.5042800321075844,\n",
       "  0.43966300116905904,\n",
       "  0.46122422277022335,\n",
       "  0.4140003710778644,\n",
       "  0.3887975782283334,\n",
       "  0.4148387507123118,\n",
       "  0.37180863085294097,\n",
       "  0.399761514707941],\n",
       " 35: [0.952802929117748,\n",
       "  0.9532401577447184,\n",
       "  0.9325778375034627,\n",
       "  0.9432148918306225,\n",
       "  0.9547500693982869,\n",
       "  0.8194155594416858,\n",
       "  0.8453497564204693,\n",
       "  0.7777278050496437,\n",
       "  0.8126079281544825],\n",
       " 36: [0.8657671826566914,\n",
       "  0.8582940391991042,\n",
       "  0.8290525693387149,\n",
       "  0.843004963493279,\n",
       "  0.8170128140308576,\n",
       "  0.7596398056341228,\n",
       "  0.78621556062379,\n",
       "  0.72,\n",
       "  0.758712550750908],\n",
       " 37: [0.5976792869109966,\n",
       "  0.5842128556385034,\n",
       "  0.5205410262420137,\n",
       "  0.542526645151363,\n",
       "  0.4947529823341262,\n",
       "  0.45978115683706805,\n",
       "  0.4866818873668189,\n",
       "  0.4408746853723494,\n",
       "  0.47100632971679973],\n",
       " 38: [0.9683385290809815,\n",
       "  0.9678754143454813,\n",
       "  0.9480640199932525,\n",
       "  0.959953683735064,\n",
       "  0.9699354516654648,\n",
       "  0.8375325711631043,\n",
       "  0.8602375830420668,\n",
       "  0.791681200153675,\n",
       "  0.8273615954517376],\n",
       " 39: [0.7582614483126875,\n",
       "  0.7509823193607029,\n",
       "  0.6931421568949785,\n",
       "  0.7124712893852687,\n",
       "  0.6892117862731529,\n",
       "  0.5904955298579553,\n",
       "  0.6183443778532187,\n",
       "  0.5625221581164963,\n",
       "  0.5952584107968104],\n",
       " 40: [0.07657277000103381,\n",
       "  0.053601039940167114,\n",
       "  0.022747083823991588,\n",
       "  0.040122355162004894,\n",
       "  0.0281892552178607,\n",
       "  0.031764605133839253,\n",
       "  0.04515433694000753,\n",
       "  0.04786965180250285,\n",
       "  0.04942158303505338],\n",
       " 41: [0.7289299009208232,\n",
       "  0.7198175919757368,\n",
       "  0.6622850992814207,\n",
       "  0.6820794205883605,\n",
       "  0.6481032209936436,\n",
       "  0.5772779617984493,\n",
       "  0.6046423135464231,\n",
       "  0.5497476216969177,\n",
       "  0.5833376863166618],\n",
       " 42: [0.5344737080353079,\n",
       "  0.5199063782575329,\n",
       "  0.4522865444127744,\n",
       "  0.4739028949704755,\n",
       "  0.43182611151239897,\n",
       "  0.38933601609657936,\n",
       "  0.4157953269470619,\n",
       "  0.37317323283459897,\n",
       "  0.40058389284241736],\n",
       " 43: [0.6455494859568364,\n",
       "  0.635096941649928,\n",
       "  0.568176751030369,\n",
       "  0.5904849390499304,\n",
       "  0.558439787685752,\n",
       "  0.47658526554775693,\n",
       "  0.504003719823672,\n",
       "  0.4565981744222567,\n",
       "  0.4854202401372214],\n",
       " 44: [0.896449758945643,\n",
       "  0.8930408083046256,\n",
       "  0.8645433617035457,\n",
       "  0.8770786296521085,\n",
       "  0.8723492146101988,\n",
       "  0.7635814889336016,\n",
       "  0.7892570930027331,\n",
       "  0.7246226582065926,\n",
       "  0.7615780445969123],\n",
       " 45: [0.9289809732484872,\n",
       "  0.9238090556901883,\n",
       "  0.9084627842765136,\n",
       "  0.9178547545311931,\n",
       "  0.9021404658077419,\n",
       "  0.8309721904046157,\n",
       "  0.8569358139880331,\n",
       "  0.7841933663121987,\n",
       "  0.8224241932113275],\n",
       " 46: [0.7625746521978966,\n",
       "  0.7491074236525403,\n",
       "  0.700553331459201,\n",
       "  0.7217287114486597,\n",
       "  0.6656728470190661,\n",
       "  0.6443474316262173,\n",
       "  0.6728520666443569,\n",
       "  0.6088888888888889,\n",
       "  0.6480723352674825],\n",
       " 47: [0.3078700660952958,\n",
       "  0.2836820227099304,\n",
       "  0.23970193582712193,\n",
       "  0.25865747895414865,\n",
       "  0.20128640326361819,\n",
       "  0.26219115681756017,\n",
       "  0.2834865987433641,\n",
       "  0.25480832944741794,\n",
       "  0.2753659394284864],\n",
       " 48: [0.8321167154442073,\n",
       "  0.8246813671283462,\n",
       "  0.7929595374156914,\n",
       "  0.8064515268852692,\n",
       "  0.785309284064773,\n",
       "  0.7162517739070586,\n",
       "  0.7422750520120286,\n",
       "  0.6772049980788392,\n",
       "  0.7122054279096129],\n",
       " 49: [0.28855722715064896,\n",
       "  0.270717358185156,\n",
       "  0.20970074288288318,\n",
       "  0.22957351174085777,\n",
       "  0.20523989950557944,\n",
       "  0.17161303821411694,\n",
       "  0.19216133942161343,\n",
       "  0.17331951310527693,\n",
       "  0.18731272695367998],\n",
       " 50: [0.5822027751778013,\n",
       "  0.5703271893616177,\n",
       "  0.49965854965324363,\n",
       "  0.5216459061632192,\n",
       "  0.4793707546300132,\n",
       "  0.42349072460147685,\n",
       "  0.4504356649768378,\n",
       "  0.4074371778663161,\n",
       "  0.43739279588336183],\n",
       " 51: [0.4014549563165948,\n",
       "  0.38388168468553374,\n",
       "  0.32574849354125074,\n",
       "  0.3456640654676094,\n",
       "  0.3108593953297867,\n",
       "  0.28634628655256805,\n",
       "  0.30936073059360736,\n",
       "  0.27780470204361907,\n",
       "  0.2990524302700512],\n",
       " 52: [0.3266166970055361,\n",
       "  0.31181947789251696,\n",
       "  0.2400652966380114,\n",
       "  0.26101141694122093,\n",
       "  0.23654088930802428,\n",
       "  0.18129198805138755,\n",
       "  0.20294353779177046,\n",
       "  0.1840530154961244,\n",
       "  0.1999484797337324],\n",
       " 53: [0.49503439511925074,\n",
       "  0.4796856721540272,\n",
       "  0.4121987062793205,\n",
       "  0.4344457882656634,\n",
       "  0.3946669809548369,\n",
       "  0.3535342749342466,\n",
       "  0.3789954337899543,\n",
       "  0.34034579157599226,\n",
       "  0.3645424786218829],\n",
       " 54: [0.26337498045696106,\n",
       "  0.2477188623049835,\n",
       "  0.1835850666220922,\n",
       "  0.20291210007816582,\n",
       "  0.18460079394735943,\n",
       "  0.1342004653621581,\n",
       "  0.15386389315534715,\n",
       "  0.14108558560939705,\n",
       "  0.15278078651992053],\n",
       " 55: [0.6769387707758985,\n",
       "  0.6670361939589312,\n",
       "  0.6021275278429404,\n",
       "  0.6244587926908929,\n",
       "  0.5907499750369436,\n",
       "  0.5094485045569731,\n",
       "  0.5369806566705035,\n",
       "  0.4873334847105953,\n",
       "  0.5180102915951975],\n",
       " 56: [0.8557899475866689,\n",
       "  0.8511173154508462,\n",
       "  0.8094206045495297,\n",
       "  0.8247520871041751,\n",
       "  0.8138426046398736,\n",
       "  0.7010619771202389,\n",
       "  0.728408457261649,\n",
       "  0.6655594381582209,\n",
       "  0.7016178352127909],\n",
       " 57: [0.19655953730264422,\n",
       "  0.1788883635106175,\n",
       "  0.12857408852240815,\n",
       "  0.1454835813252978,\n",
       "  0.13490967521351563,\n",
       "  0.09827321836737399,\n",
       "  0.11516375504417928,\n",
       "  0.10776759349345838,\n",
       "  0.11820163788240298],\n",
       " 58: [0.9334809936751309,\n",
       "  0.9317043141810364,\n",
       "  0.9072787425377995,\n",
       "  0.9181411955857839,\n",
       "  0.9225544160016242,\n",
       "  0.7962703860706664,\n",
       "  0.8219740946020454,\n",
       "  0.7549599177300613,\n",
       "  0.7923568503843591],\n",
       " 59: [0.9535853276341286,\n",
       "  0.949906214810256,\n",
       "  0.9322775074187747,\n",
       "  0.9426867198433374,\n",
       "  0.9341255106696555,\n",
       "  0.8438915314222789,\n",
       "  0.8698630136986305,\n",
       "  0.7988990768992846,\n",
       "  0.8400389908900562],\n",
       " 60: [0.4956556976188481,\n",
       "  0.4813332294402867,\n",
       "  0.4140196440534565,\n",
       "  0.43595167263225904,\n",
       "  0.40127870164756235,\n",
       "  0.348826029309205,\n",
       "  0.37395884149582514,\n",
       "  0.3369967130610858,\n",
       "  0.36045542158691757],\n",
       " 61: [0.937837309069902,\n",
       "  0.9296307592360366,\n",
       "  0.9204745859485157,\n",
       "  0.9295295561268778,\n",
       "  0.8973981373758262,\n",
       "  0.8732394366197183,\n",
       "  0.8998581218249222,\n",
       "  0.8226400924539168,\n",
       "  0.8639686778716079],\n",
       " 62: [0.7068604765861157,\n",
       "  0.6885785609495754,\n",
       "  0.6444266871600223,\n",
       "  0.6654585671090021,\n",
       "  0.5903463743475506,\n",
       "  0.6280202176347764,\n",
       "  0.656773211567732,\n",
       "  0.5926189260641629,\n",
       "  0.632427578785525],\n",
       " 63: [0.9566831975354997,\n",
       "  0.9524766628443079,\n",
       "  0.9350993903974087,\n",
       "  0.9459403970144812,\n",
       "  0.9435424866651183,\n",
       "  0.839195465983568,\n",
       "  0.8654766772375941,\n",
       "  0.7923301777031239,\n",
       "  0.831274239057905],\n",
       " 64: [0.11745237903025728,\n",
       "  0.09594892485322325,\n",
       "  0.05780206867702447,\n",
       "  0.0766746123324935,\n",
       "  0.0642627174224353,\n",
       "  0.050301810865191116,\n",
       "  0.06494251574997456,\n",
       "  0.0648848670953528,\n",
       "  0.06890815642318504],\n",
       " 65: [0.8798338523480067,\n",
       "  0.8739413321812096,\n",
       "  0.8425252355781132,\n",
       "  0.855666326231636,\n",
       "  0.8429523795076332,\n",
       "  0.7504069183784379,\n",
       "  0.7773487748883766,\n",
       "  0.7093480963701861,\n",
       "  0.746884878062019],\n",
       " 66: [0.6746805488464694,\n",
       "  0.6651260100940551,\n",
       "  0.6003788380677862,\n",
       "  0.6219765300263115,\n",
       "  0.5858537649816855,\n",
       "  0.5092782005423566,\n",
       "  0.5366488579766986,\n",
       "  0.4873993627066038,\n",
       "  0.5201968988024606],\n",
       " 67: [0.9655799262498289,\n",
       "  0.9638044294877851,\n",
       "  0.9562679274532481,\n",
       "  0.962715446354854,\n",
       "  0.9715717014620237,\n",
       "  0.8671103806937018,\n",
       "  0.8914008924760829,\n",
       "  0.817602889121042,\n",
       "  0.8523445728917198],\n",
       " 68: [0.7256765591103626,\n",
       "  0.7102430497222628,\n",
       "  0.6614324054920351,\n",
       "  0.681716321126972,\n",
       "  0.6187373317167719,\n",
       "  0.62642456822965,\n",
       "  0.6548706240487061,\n",
       "  0.5916730062940372,\n",
       "  0.6297327389317018],\n",
       " 69: [0.805281734741451,\n",
       "  0.7976938794506779,\n",
       "  0.7528545972778641,\n",
       "  0.7695647191865096,\n",
       "  0.7402431610232882,\n",
       "  0.6703655435884788,\n",
       "  0.6976279985676741,\n",
       "  0.6369716401386435,\n",
       "  0.6741909367117135],\n",
       " 70: [0.34655884957142025,\n",
       "  0.33124963323979767,\n",
       "  0.26107494437016276,\n",
       "  0.28140720368176375,\n",
       "  0.2548686466043457,\n",
       "  0.2039777840362146,\n",
       "  0.22652447905567552,\n",
       "  0.20387374675729672,\n",
       "  0.2217338867321998],\n",
       " 71: [0.29215194860564675,\n",
       "  0.2756548898849955,\n",
       "  0.21252348784194977,\n",
       "  0.23173824970579066,\n",
       "  0.21077339395049724,\n",
       "  0.1657387588774315,\n",
       "  0.1864535768645357,\n",
       "  0.16873325441762027,\n",
       "  0.18369507407376698],\n",
       " 72: [0.1276461606263727,\n",
       "  0.10679658178603951,\n",
       "  0.06697079120987035,\n",
       "  0.08583211734542212,\n",
       "  0.07471520656992627,\n",
       "  0.05432595573440642,\n",
       "  0.06917900259221163,\n",
       "  0.06861460271811114,\n",
       "  0.07313067380220015],\n",
       " 73: [0.11417194399830492,\n",
       "  0.09223818387939492,\n",
       "  0.05521192147149177,\n",
       "  0.0740827220135401,\n",
       "  0.0604132912842736,\n",
       "  0.05130784708249491,\n",
       "  0.06589166925120839,\n",
       "  0.06578117766478438,\n",
       "  0.06976761432802636],\n",
       " 74: [0.8770829274521572,\n",
       "  0.8706000436360746,\n",
       "  0.838508378724488,\n",
       "  0.8521908663175838,\n",
       "  0.836410673506324,\n",
       "  0.7485910221853009,\n",
       "  0.7757855347583499,\n",
       "  0.7073180721557282,\n",
       "  0.7451882597354555],\n",
       " 75: [0.22512508599094008,\n",
       "  0.20857269445921656,\n",
       "  0.15172042985979922,\n",
       "  0.16911184297822782,\n",
       "  0.15345353299967437,\n",
       "  0.10962367645609417,\n",
       "  0.12857536033692116,\n",
       "  0.1180880467064181,\n",
       "  0.12658292954639766],\n",
       " 76: [0.8729546836733094,\n",
       "  0.8619846705546971,\n",
       "  0.839634795665263,\n",
       "  0.8527812881935578,\n",
       "  0.8073008986385642,\n",
       "  0.7985828737285419,\n",
       "  0.8261035007610349,\n",
       "  0.7523388686277798,\n",
       "  0.7939093899996781],\n",
       " 77: [0.32123798448392665,\n",
       "  0.3037219551050512,\n",
       "  0.2479651074987711,\n",
       "  0.2673006969418718,\n",
       "  0.24473518693339139,\n",
       "  0.20925553319919513,\n",
       "  0.22982021677586034,\n",
       "  0.20825581304699944,\n",
       "  0.22326377507913223],\n",
       " 78: [0.8377878129510673,\n",
       "  0.8312878915216675,\n",
       "  0.793135475357958,\n",
       "  0.8083831850529355,\n",
       "  0.7882919099190981,\n",
       "  0.7032193158953726,\n",
       "  0.7298300915719019,\n",
       "  0.6664429492085127,\n",
       "  0.7036973969608079],\n",
       " 79: [0.4774320704861761,\n",
       "  0.463826188261773,\n",
       "  0.4039798058966019,\n",
       "  0.4238853400642539,\n",
       "  0.3920437638497812,\n",
       "  0.3435187713171541,\n",
       "  0.36719939117199396,\n",
       "  0.33415441218612374,\n",
       "  0.35858394726776793],\n",
       " 80: [0.654648067403408,\n",
       "  0.6347605970946707,\n",
       "  0.5902436129562059,\n",
       "  0.6108022645530515,\n",
       "  0.531732408517054,\n",
       "  0.586731184125981,\n",
       "  0.6150847723980435,\n",
       "  0.5540743759262944,\n",
       "  0.5929302096066368],\n",
       " 81: [0.8002866330229527,\n",
       "  0.78549545135311,\n",
       "  0.7536048719264266,\n",
       "  0.7701000932181576,\n",
       "  0.7100551599077207,\n",
       "  0.731015574324091,\n",
       "  0.7590591945456713,\n",
       "  0.688888888888889,\n",
       "  0.7294434815429099],\n",
       " 82: [0.8236498134451117,\n",
       "  0.8088564623377086,\n",
       "  0.7827283002983292,\n",
       "  0.7978655673201671,\n",
       "  0.7372527284840237,\n",
       "  0.7665995975855131,\n",
       "  0.7946108638748565,\n",
       "  0.7218931490633499,\n",
       "  0.7632665381304242],\n",
       " 83: [0.8288193328836643,\n",
       "  0.8231955815975365,\n",
       "  0.7817335299771387,\n",
       "  0.7973094582995326,\n",
       "  0.7805272806410539,\n",
       "  0.6844186754037893,\n",
       "  0.7109512556471487,\n",
       "  0.649954406966743,\n",
       "  0.6861063464837049],\n",
       " 84: [0.8288760480459297,\n",
       "  0.8203009445738805,\n",
       "  0.7840634940695994,\n",
       "  0.7993790923777083,\n",
       "  0.7683097169889483,\n",
       "  0.7089830253948755,\n",
       "  0.7360558704970496,\n",
       "  0.6699499729632942,\n",
       "  0.7077091741177644],\n",
       " 85: [0.6173666003357561,\n",
       "  0.604670751666583,\n",
       "  0.5335390505909011,\n",
       "  0.5566137433343529,\n",
       "  0.5132601990984442,\n",
       "  0.45573440643863183,\n",
       "  0.4837691272500016,\n",
       "  0.4351675946756013,\n",
       "  0.4645914737797304],\n",
       " 86: [0.8705800236201588,\n",
       "  0.8599812424755788,\n",
       "  0.8397356586911863,\n",
       "  0.8527668876599975,\n",
       "  0.8120455608935274,\n",
       "  0.796558779214491,\n",
       "  0.8234255868220921,\n",
       "  0.7511111111111111,\n",
       "  0.7901702752295422],\n",
       " 87: [0.7610041614924302,\n",
       "  0.7514725728485745,\n",
       "  0.7012777582815126,\n",
       "  0.7193613792565098,\n",
       "  0.6900729669775706,\n",
       "  0.6192491040631609,\n",
       "  0.6468213472895309,\n",
       "  0.5867357961582252,\n",
       "  0.6203488443140901],\n",
       " 88: [0.35350757653070186,\n",
       "  0.33843915358704557,\n",
       "  0.26666366815178366,\n",
       "  0.28852312447965206,\n",
       "  0.26091063946628007,\n",
       "  0.20289199126956495,\n",
       "  0.22610128068264243,\n",
       "  0.20491931356876963,\n",
       "  0.21821065023065706],\n",
       " 89: [0.848976276146874,\n",
       "  0.8369351159116715,\n",
       "  0.8093843911583761,\n",
       "  0.8240755881469181,\n",
       "  0.7757476110403834,\n",
       "  0.7702285500567729,\n",
       "  0.7979409108302272,\n",
       "  0.7259813349059857,\n",
       "  0.7684391080617495],\n",
       " 90: [0.9741926292046811,\n",
       "  0.9669249386326969,\n",
       "  0.9689968749084386,\n",
       "  0.9743136889147086,\n",
       "  0.9491833369702076,\n",
       "  0.9305835010060362,\n",
       "  0.9563927608366491,\n",
       "  0.8761319903675013,\n",
       "  0.9176672384219554],\n",
       " 91: [0.9274330647623148,\n",
       "  0.922980896402662,\n",
       "  0.9013366511182093,\n",
       "  0.9123179889867398,\n",
       "  0.9085748159488799,\n",
       "  0.806723148926212,\n",
       "  0.8329528158295283,\n",
       "  0.7620755783082335,\n",
       "  0.8000321931999292],\n",
       " 92: [0.4357278218484615,\n",
       "  0.421939449241563,\n",
       "  0.3460287328645,\n",
       "  0.3682473804242431,\n",
       "  0.33628085165637944,\n",
       "  0.27565392354124735,\n",
       "  0.3003292968707178,\n",
       "  0.27012648916286974,\n",
       "  0.29085502724942436],\n",
       " 93: [0.7301769699181266,\n",
       "  0.7191371025108995,\n",
       "  0.6646951324542798,\n",
       "  0.6845150875238433,\n",
       "  0.6409998106696784,\n",
       "  0.5965975424880268,\n",
       "  0.6243192873521071,\n",
       "  0.5668459396087131,\n",
       "  0.6020583190394511],\n",
       " 94: [0.432426842149234,\n",
       "  0.411354885560547,\n",
       "  0.35845859313112366,\n",
       "  0.379252657567038,\n",
       "  0.31865903195353823,\n",
       "  0.35446876785221815,\n",
       "  0.3786149162861492,\n",
       "  0.3399848312484296,\n",
       "  0.365798734403564],\n",
       " 95: [0.24976119303096278,\n",
       "  0.22506462412053985,\n",
       "  0.1862289541355843,\n",
       "  0.2041041646563856,\n",
       "  0.1547471711147389,\n",
       "  0.21320716414501792,\n",
       "  0.23259069513847458,\n",
       "  0.21000663499749653,\n",
       "  0.22715132639919047],\n",
       " 96: [0.6713664802318337,\n",
       "  0.6611925477050375,\n",
       "  0.596923302374304,\n",
       "  0.619194347749433,\n",
       "  0.5850275916229756,\n",
       "  0.5064571273129498,\n",
       "  0.5338442713943413,\n",
       "  0.4844444444444443,\n",
       "  0.515013635482571],\n",
       " 97: [0.580812172548334,\n",
       "  0.5656938517702635,\n",
       "  0.5094718242790103,\n",
       "  0.531055082856539,\n",
       "  0.4882836248472805,\n",
       "  0.454728370221328,\n",
       "  0.48049843207773907,\n",
       "  0.43370683222718903,\n",
       "  0.4621575805006997],\n",
       " 98: [0.06550940397403693,\n",
       "  0.043921606293386574,\n",
       "  0.009111666517807171,\n",
       "  0.02665996437818085,\n",
       "  0.021238956641068504,\n",
       "  0.004810299737749684,\n",
       "  0.0178843226788432,\n",
       "  0.02323279070850227,\n",
       "  0.023081301727151057],\n",
       " 99: [0.30794736043623927,\n",
       "  0.29279735495402764,\n",
       "  0.2232148408316524,\n",
       "  0.24375259953945114,\n",
       "  0.2211383685904752,\n",
       "  0.1677279764505698,\n",
       "  0.1887714963016872,\n",
       "  0.17159689104421905,\n",
       "  0.1865193478301046]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('calibrate-simulation')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "357fe205416135b56b7cab8cdb5fc558aca32511d46f934dc204bc71940a4b8a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
